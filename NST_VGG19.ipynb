{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp3FO4XD4KOi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras.preprocessing.image as process_im\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications import vgg19\n",
        "from keras.models import Model\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import backend as K\n",
        "import functools\n",
        "import IPython.display\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image = cv2.imread(\"1.jpg\")\n",
        "\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "resized_image = cv2.resize(gray_image, (300, 300))\n",
        "\n",
        "# 4. Enhance image contrast using histogram equalization\n",
        "equalized_image = cv2.equalizeHist(resized_image)\n",
        "\n",
        "# 5. Edge Detection using Canny Edge Detector\n",
        "edges = cv2.Canny(equalized_image, 100, 200)\n",
        "\n",
        "# 6. Thresholding the image\n",
        "_, thresh_image = cv2.threshold(equalized_image, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "contours, _ = cv2.findContours(thresh_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "image_with_contours = cv2.drawContours(image.copy(), contours, -1, (0, 255, 0), 2)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2, 3, 1), plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)), plt.title('Original Image')\n",
        "plt.subplot(2, 3, 2), plt.imshow(gray_image, cmap='gray'), plt.title('Grayscale Image')\n",
        "plt.subplot(2, 3, 3), plt.imshow(resized_image, cmap='gray'), plt.title('Resized Image')\n",
        "plt.subplot(2, 3, 4), plt.imshow(equalized_image, cmap='gray'), plt.title('Enhanced Image')\n",
        "plt.subplot(2, 3, 5), plt.imshow(edges, cmap='gray'), plt.title('Edge Detection')\n",
        "plt.subplot(2, 3, 6), plt.imshow(cv2.cvtColor(image_with_contours, cv2.COLOR_BGR2RGB)), plt.title('Contours')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lZ_ewE0MJRkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0uzB4JR8MOX"
      },
      "outputs": [],
      "source": [
        "content_path='/content/1.jpg'\n",
        "style_path = '/content/2.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QWT378t5SFM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_file(image_path):\n",
        "    image =  Image.open(image_path)\n",
        "    max_dim=512\n",
        "    factor=max_dim/max(image.size)\n",
        "    image=image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),Image.LANCZOS)\n",
        "    im_array = process_im.img_to_array(image)\n",
        "    im_array = np.expand_dims(im_array,axis=0) #adding extra axis to the array as to generate a\n",
        "                                               #batch of single image\n",
        "\n",
        "    return im_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Ad5lix5ZIW"
      },
      "outputs": [],
      "source": [
        "def show_im(img,title=None):\n",
        "    img=np.squeeze(img,axis=0) #squeeze array to drop batch axis\n",
        "    plt.imshow(np.uint8(img))\n",
        "    if title is None:\n",
        "        pass\n",
        "    else:\n",
        "        plt.title(title)\n",
        "    plt.imshow(np.uint8(img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBLPxbPF5cbR"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "content = load_file(content_path)\n",
        "style = load_file(style_path)\n",
        "plt.subplot(1,2,1)\n",
        "show_im(content,'Content Image')\n",
        "plt.subplot(1,2,2)\n",
        "show_im(style,'Style Image')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mLHoSN65gSz"
      },
      "outputs": [],
      "source": [
        "def img_preprocess(img_path):\n",
        "    image=load_file(img_path)\n",
        "    img=tf.keras.applications.vgg19.preprocess_input(image)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX77Bmse5g5C"
      },
      "outputs": [],
      "source": [
        "def deprocess_img(processed_img):\n",
        "  x = processed_img.copy()\n",
        "  if len(x.shape) == 4:\n",
        "    x = np.squeeze(x, 0)\n",
        "  assert len(x.shape) == 3 #Input dimension must be [1, height, width, channel] or [height, width, channel]\n",
        "\n",
        "\n",
        "  # perform the inverse of the preprocessing step\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1] # converting BGR to RGB channel\n",
        "\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf2vbKh65zX7"
      },
      "outputs": [],
      "source": [
        "im=img_preprocess(content_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOZew2Ff514O"
      },
      "outputs": [],
      "source": [
        "content_layers = ['block5_conv2']\n",
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
        "number_content=len(content_layers)\n",
        "number_style =len(style_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLfEFklU55P9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "\n",
        "# Define the content and style layers\n",
        "content_layers = ['block5_conv2']\n",
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
        "\n",
        "def get_model():\n",
        "    # Load the VGG19 model with pre-trained ImageNet weights\n",
        "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "\n",
        "    # Create lists of style and content layer outputs\n",
        "    content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
        "    style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
        "\n",
        "    # Combine the style and content outputs\n",
        "    model_outputs = style_outputs + content_outputs\n",
        "\n",
        "    # Return a new model with the VGG19 inputs and the outputs from the selected layers\n",
        "    return models.Model(inputs=vgg.input, outputs=model_outputs)\n",
        "\n",
        "# Call the function and print model summary\n",
        "model = get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh3vGhPW58-s"
      },
      "outputs": [],
      "source": [
        "def get_content_loss(noise,target):\n",
        "    loss = tf.reduce_mean(tf.square(noise-target))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWwAsCPr6c3b"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "    channels=int(tensor.shape[-1])\n",
        "    vector=tf.reshape(tensor,[-1,channels])\n",
        "    n=tf.shape(vector)[0]\n",
        "    gram_matrix=tf.matmul(vector,vector,transpose_a=True)\n",
        "    return gram_matrix/tf.cast(n,tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j8_qhfZ6fWg"
      },
      "outputs": [],
      "source": [
        "def get_style_loss(noise,target):\n",
        "    gram_noise=gram_matrix(noise)\n",
        "    #gram_target=gram_matrix(target)\n",
        "    loss=tf.reduce_mean(tf.square(target-gram_noise))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9ykzuAe6hmT"
      },
      "outputs": [],
      "source": [
        "def get_features(model,content_path,style_path):\n",
        "    content_img=img_preprocess(content_path)\n",
        "    style_image=img_preprocess(style_path)\n",
        "\n",
        "    content_output=model(content_img)\n",
        "    style_output=model(style_image)\n",
        "\n",
        "    content_feature = [layer[0] for layer in content_output[number_style:]]\n",
        "    style_feature = [layer[0] for layer in style_output[:number_style]]\n",
        "    return content_feature,style_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGvHbOSg6kTi"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, loss_weights,image, gram_style_features, content_features):\n",
        "    style_weight,content_weight = loss_weights #style weight and content weight are user given parameters\n",
        "                                               #that define what percentage of content and/or style will be preserved in the generated image\n",
        "\n",
        "    output=model(image)\n",
        "    content_loss=0\n",
        "    style_loss=0\n",
        "\n",
        "    noise_style_features = output[:number_style]\n",
        "    noise_content_feature = output[number_style:]\n",
        "\n",
        "    weight_per_layer = 1.0/float(number_style)\n",
        "    for a,b in zip(gram_style_features,noise_style_features):\n",
        "        style_loss+=weight_per_layer*get_style_loss(b[0],a)\n",
        "\n",
        "\n",
        "    weight_per_layer =1.0/ float(number_content)\n",
        "    for a,b in zip(noise_content_feature,content_features):\n",
        "        content_loss+=weight_per_layer*get_content_loss(a[0],b)\n",
        "\n",
        "    style_loss *= style_weight\n",
        "    content_loss *= content_weight\n",
        "\n",
        "    total_loss = content_loss + style_loss\n",
        "\n",
        "\n",
        "    return total_loss,style_loss,content_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WjaGtoZ6y2X"
      },
      "outputs": [],
      "source": [
        "def compute_grads(dictionary):\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_loss=compute_loss(**dictionary)\n",
        "\n",
        "    total_loss=all_loss[0]\n",
        "    return tape.gradient(total_loss,dictionary['image']),all_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jvvMvQh67w6"
      },
      "outputs": [],
      "source": [
        "def run_style_transfer(content_path,style_path,epochs=1000,content_weight=1e3, style_weight=1e-2):\n",
        "\n",
        "    model=get_model()\n",
        "\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    content_feature,style_feature = get_features(model,content_path,style_path)\n",
        "    style_gram_matrix=[gram_matrix(feature) for feature in style_feature]\n",
        "\n",
        "    noise = img_preprocess(content_path)\n",
        "    noise=tf.Variable(noise,dtype=tf.float32)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5.0, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "    best_loss,best_img=float('inf'),None\n",
        "\n",
        "    loss_weights = (style_weight, content_weight)\n",
        "    dictionary={'model':model,\n",
        "              'loss_weights':loss_weights,\n",
        "              'image':noise,\n",
        "              'gram_style_features':style_gram_matrix,\n",
        "              'content_features':content_feature}\n",
        "\n",
        "    norm_means = np.array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means\n",
        "\n",
        "    imgs = []\n",
        "    for i in range(epochs):\n",
        "        grad,all_loss=compute_grads(dictionary)\n",
        "        total_loss,style_loss,content_loss=all_loss\n",
        "        optimizer.apply_gradients([(grad,noise)])\n",
        "        clipped=tf.clip_by_value(noise,min_vals,max_vals)\n",
        "        noise.assign(clipped)\n",
        "\n",
        "        if total_loss<best_loss:\n",
        "            best_loss = total_loss\n",
        "            best_img = deprocess_img(noise.numpy())\n",
        "\n",
        "         #for visualization\n",
        "\n",
        "        if i%5==0:\n",
        "            plot_img = noise.numpy()\n",
        "            plot_img = deprocess_img(plot_img)\n",
        "            imgs.append(plot_img)\n",
        "            IPython.display.clear_output(wait=True)\n",
        "            IPython.display.display_png(Image.fromarray(plot_img))\n",
        "            print('Epoch: {}'.format(i))\n",
        "            print('Total loss: {:.4e}, '\n",
        "              'style loss: {:.4e}, '\n",
        "              'content loss: {:.4e}, '.format(total_loss, style_loss, content_loss))\n",
        "\n",
        "    IPython.display.clear_output(wait=True)\n",
        "\n",
        "\n",
        "    return best_img,best_loss,imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQFk6jyK7BzH"
      },
      "outputs": [],
      "source": [
        "best, best_loss,image = run_style_transfer(content_path, style_path, epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XXcNzeU7Eag"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(best)\n",
        "plt.title('Style transfer Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,1)\n",
        "show_im(content,'Content Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,2)\n",
        "show_im(style,'Style Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dz0Hrga2Ks8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}